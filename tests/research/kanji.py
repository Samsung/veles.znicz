#!/usr/bin/python3 -O
# encoding: utf-8
"""
Created on June 29, 2013


Model created for Chinese characters recognition. Dataset was generated by
VELES with generate_kanji.py utility.
Model â€“ fully-connected Neural Network with MSE loss function.

Copyright (c) 2013 Samsung Electronics Co., Ltd.
"""


import logging
import numpy
import os
import pickle
import re
from zope.interface import implementer

from veles.config import root
import veles.error as error
import veles.formats as formats
import veles.opencl_types as opencl_types
import veles.prng as rnd
import veles.znicz.loader as loader
from veles.znicz.standard_workflow import StandardWorkflow


@implementer(loader.ILoader)
class KanjiLoader(loader.LoaderMSE):
    """Loads dataset.
    """
    def __init__(self, workflow, **kwargs):
        self.train_path = kwargs["train_path"]
        self.target_path = kwargs["target_path"]
        super(KanjiLoader, self).__init__(workflow, **kwargs)
        self.class_targets = formats.Vector()

    def __getstate__(self):
        state = super(KanjiLoader, self).__getstate__()
        state["index_map"] = None
        return state

    def load_data(self):
        """Load the data here.

        Should be filled here:
            class_lengths[].
        """
        fin = open(root.kanji_standard.index_map, "rb")
        self.index_map = pickle.load(fin)
        fin.close()

        fin = open(os.path.join(self.train_path, self.index_map[0]), "rb")
        self.first_sample = pickle.load(fin)["data"]
        fin.close()

        fin = open(self.target_path, "rb")
        targets = pickle.load(fin)
        fin.close()
        self.class_targets.reset()
        sh = [len(targets)]
        sh.extend(targets[0].shape)
        self.class_targets.mem = numpy.empty(
            sh, dtype=opencl_types.dtypes[root.common.precision_type])
        for i, target in enumerate(targets):
            self.class_targets[i] = target

        self.class_lengths[0] = 0
        self.class_lengths[1] = 0
        self.class_lengths[2] = len(self.index_map)

        self.original_labels = numpy.empty(len(self.index_map),
                                           dtype=numpy.int32)
        lbl_re = re.compile("^(\d+)_\d+/(\d+)\.\d\.pickle$")
        for i, fnme in enumerate(self.index_map):
            res = lbl_re.search(fnme)
            if res is None:
                raise error.BadFormatError("Incorrectly formatted filename "
                                           "found: %s" % (fnme))
            lbl = int(res.group(1))
            self.original_labels[i] = lbl
            idx = int(res.group(2))
            if idx != i:
                raise error.BadFormatError("Incorrect sample index extracted "
                                           "from filename: %s " % (fnme))

        self.info("Found %d samples. Extracting 15%% for validation..." % (
            len(self.index_map)))
        self.extract_validation_from_train(rnd.get(2))
        self.info("Extracted, resulting datasets are: [%s]" % (
            ", ".join(str(x) for x in self.class_lengths)))

    def create_minibatches(self):
        """Allocate arrays for minibatch_data etc. here.
        """
        sh = [self.max_minibatch_size]
        sh.extend(self.first_sample.shape)
        self.minibatch_data.mem = numpy.zeros(
            sh, dtype=opencl_types.dtypes[root.common.precision_type])

        sh = [self.max_minibatch_size]
        sh.extend((self.class_targets[0].size,))
        self.minibatch_targets.mem = numpy.zeros(
            sh, dtype=opencl_types.dtypes[root.common.precision_type])

        sh = [self.max_minibatch_size]
        self.minibatch_labels.mem = numpy.zeros(sh, dtype=numpy.int32)

        self.minibatch_indices.mem = numpy.zeros(len(self.index_map),
                                                 dtype=numpy.int32)

    def fill_minibatch(self):
        """Fill minibatch data labels and indexes according to current shuffle.
        """
        idxs = self.minibatch_indices.mem
        for i, ii in enumerate(idxs[:self.minibatch_size]):
            fnme = "%s/%s" % (self.train_path, self.index_map[ii])
            fin = open(fnme, "rb")
            sample = pickle.load(fin)
            data = sample["data"]
            lbl = sample["lbl"]
            fin.close()
            self.minibatch_data[i] = data
            self.minibatch_labels[i] = lbl
            self.minibatch_targets[i] = self.class_targets[lbl].reshape(
                self.minibatch_targets[i].shape)


class KanjiWorkflow(StandardWorkflow):
    def __init__(self, workflow, **kwargs):
        super(KanjiWorkflow, self).__init__(
            workflow,
            fail_iterations=root.kanji_standard.decision.fail_iterations,
            max_epochs=root.kanji_standard.decision.max_epochs,
            prefix=root.kanji_standard.snapshotter.prefix,
            snapshot_dir=root.common.snapshot_dir,
            layers=root.kanji_standard.layers,
            loss_function=root.kanji_standard.loss_function, **kwargs)

    def initialize(self, device, weights, bias, **kwargs):
        super(KanjiWorkflow, self).initialize(device=device)
        if weights is not None:
            for i, fwds in enumerate(self.fwds):
                fwds.weights.map_invalidate()
                fwds.weights.mem[:] = weights[i][:]
        if bias is not None:
            for i, fwds in enumerate(self.fwds):
                fwds.bias.map_invalidate()
                fwds.bias.mem[:] = bias[i][:]

    def link_loader(self):
        super(KanjiWorkflow, self).link_loader()
        self.loader = KanjiLoader(
            self, validation_ratio=root.kanji_standard.loader.validation_ratio,
            train_path=root.kanji_standard.data_paths.train,
            target_path=root.kanji_standard.data_paths.target,
            minibatch_size=root.kanji_standard.loader.minibatch_size)
        self.loader.link_from(self.repeater)


def run(load, main):
    weights = None
    bias = None
    w, snapshot = load(KanjiWorkflow)
    if snapshot:
        if type(w) == tuple:
            logging.info("Will load weights")
            weights = w[0]
            bias = w[1]
        else:
            logging.info("Will load workflow")
            logging.info("Weights and bias ranges per layer are:")
            for fwds in w.fwds:
                logging.info("%f %f %f %f" % (
                    fwds.weights.mem.min(), fwds.weights.mem.max(),
                    fwds.bias.mem.min(), fwds.bias.mem.max()))
            w.decision.improved <<= True
    main(weights=weights, bias=bias)
